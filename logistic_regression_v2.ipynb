{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goldsmiths University of London\n",
    "### Author....: Carlos Manuel de Oliveira Alves\n",
    "### Student...: cdeol003\n",
    "### Created...: 11/02/2023\n",
    "### FYP.......: NeuroCredit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "dataset = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the dataset into independent and dependent variables\n",
    "\n",
    "# Select all the columns except the last one as the independent variables vector\n",
    "X = dataset.iloc[:, :-1].values\n",
    "\n",
    "# Select the last column as the dependent variable vector\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Poor' 'Self-Employed' 'None' 'Fair' 'Auto' 'Judgement'\n",
      "  'Debt Consolidation' 30971 11969 78 21 0 5 0 135 18627 25 4648 4719]\n",
      " ['Poor' 'Unemployed' 'Investment' 'Excellent' 'Mortgage' 'Collection'\n",
      "  'Home Improvement' 19120 45396 81 92 0 2 1 19857 11996 58 2657 19366]\n",
      " ['Excellent' 'Unemployed' 'House' 'Poor' 'Other' 'Bankruptcy'\n",
      "  'Debt Consolidation' 48687 18880 73 27 0 3 1 17501 18074 30 2487 15500]\n",
      " ['Fair' 'Unemployed' 'Car' 'Good' 'Other' 'Tax Lien' 'Home Improvement'\n",
      "  40997 11527 6 19 3 3 4 9376 5073 60 3100 12380]\n",
      " ['Good' 'Unemployed' 'House' 'Excellent' 'Personal' 'None' 'Business'\n",
      "  35485 27016 33 49 5 2 3 18960 14542 99 1596 7946]]\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows with the independent variables vector\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rejected' 'Rejected' 'Rejected' 'Rejected' 'Rejected']\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows with the dependent variable vector\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_history                                     0\n",
      "employment_status                                  0\n",
      "collateral                                         0\n",
      "payment_history                                    0\n",
      "type_of_credit_accounts                            0\n",
      "public_records_and_collections                     0\n",
      "purpose_of_loan                                    0\n",
      "income                                             0\n",
      "assets_value                                       0\n",
      "debt_to_income_ratio                               0\n",
      "length_of_credit_history                           0\n",
      "number_of_credit_inquiries                         0\n",
      "number_of_credit_accounts                          0\n",
      "number_of_credit_accounts_opened_last_12_months    0\n",
      "current_balance_of_credit_accounts                 0\n",
      "total_credit_limit                                 0\n",
      "total_credit_utilization                           0\n",
      "loan_amount                                        0\n",
      "saving_account_balance                             0\n",
      "approval_status                                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the missing values in the dataset\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical data\n",
    "\n",
    "# Import the library for encoding the categorical data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Import the library for one hot encoding the categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an object of the ColumnTransformer class\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0,1,2,3,4,5,6])], remainder='passthrough')\n",
    "\n",
    "# Encoding the independent variables vector\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0\n",
      "  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 30971 11969\n",
      "  78 21 0 5 0 135 18627 25 4648 4719]\n",
      " [0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "  1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 19120 45396\n",
      "  81 92 0 2 1 19857 11996 58 2657 19366]\n",
      " [1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0\n",
      "  0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 48687 18880\n",
      "  73 27 0 3 1 17501 18074 30 2487 15500]\n",
      " [0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0\n",
      "  0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 40997 11527\n",
      "  6 19 3 3 4 9376 5073 60 3100 12380]\n",
      " [0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "  0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 35485 27016\n",
      "  33 49 5 2 3 18960 14542 99 1596 7946]]\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows with the independent variables vector\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the dependent variable vector\n",
    "\n",
    "# Import the library for encoding the categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an object of the LabelEncoder class\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encoding the dependent variable vector\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows with the dependent variable vector after encoding\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "\n",
    "# Import the library for splitting the dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 36933 32921\n",
      "  12 23 5 2 1 12728 4417 33 1673 17013]\n",
      " [1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 42898 41738\n",
      "  50 21 3 5 0 10581 1886 7 4918 5022]\n",
      " [0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 12434 31547\n",
      "  32 80 0 3 2 11553 16452 33 3848 6464]\n",
      " [1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0\n",
      "  0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 18089 18164\n",
      "  78 44 1 3 5 4784 12920 59 1886 7582]\n",
      " [1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0\n",
      "  0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 15341 47687\n",
      "  87 62 2 0 3 11877 1632 22 2821 11378]]\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows from the X training set\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 14670 28260\n",
      "  79 29 4 2 0 13341 1874 22 2942 6405]\n",
      " [0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0\n",
      "  0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 20346 20062\n",
      "  90 21 1 5 0 19669 2938 59 2848 9107]\n",
      " [0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 46322 40877\n",
      "  60 94 4 3 4 1829 2499 0 3514 3353]\n",
      " [1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 41282 13191\n",
      "  38 54 5 3 3 15807 5887 92 3152 19206]\n",
      " [0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 22875 19554\n",
      "  38 83 2 2 5 12738 8179 52 2228 17945]]\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows from the X test set\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows from the Y training set\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 rows from the Y test set\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "\n",
    "# Import the library for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create an instance of the StandardScaler class\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the training set using the numerical columns\n",
    "X_train[:, 33:] = sc.fit_transform(X_train[:, 33:])\n",
    "\n",
    "# Transform the test set using the numerical columns\n",
    "X_test[:, 33:] = sc.transform(X_test[:, 33:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n",
      "  0.5870100043393323 0.2917414330328959 -1.3361545310763985\n",
      "  -0.8821090336865706 1.4910225551575205 -0.2917745192078006\n",
      "  -0.8045195835759038 0.4780163302018978 -0.9447594247583616\n",
      "  -0.5632067010101276 -1.13876683459393 1.2023332733261243]\n",
      " [1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0\n",
      "  1.0922807178278127 1.0428648809425443 -0.011417144580980268\n",
      "  -0.9510776837011578 0.3187198090877256 1.5083767561102492\n",
      "  -1.3817533655137721 0.09861672734800296 -1.379386590397935\n",
      "  -1.4805676548872344 1.6206122456678749 -0.8887099264008494]\n",
      " [0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0\n",
      "  -1.4881999218558537 0.17468987156892216 -0.6389243276577573\n",
      "  1.0834974917291653 -1.4397343100169668 0.308275905898216\n",
      "  -0.22728580163803552 0.2703803109409167 1.1219090614021547\n",
      "  -0.5632067010101276 0.7107399449343984 -0.6372476385177458]]\n"
     ]
    }
   ],
   "source": [
    "# Print first 3 rows from the X training set\n",
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0\n",
      "  -1.2987975219631056 -0.10533085807596997 0.9995666503760494\n",
      "  -0.6752030836428089 0.9048711821226231 -0.2917745192078006\n",
      "  -1.3817533655137721 0.586340483229116 -1.381447248630916\n",
      "  -0.9513209507273651 -0.059675292882919014 -0.6475363174255843]\n",
      " [0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0\n",
      "  0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0\n",
      "  -0.8180068145430527 -0.8037214700713535 1.383043262256302\n",
      "  -0.9510776837011578 -0.8535829369820693 1.5083767561102492\n",
      "  -1.3817533655137721 1.7045708916405957 -1.1987355519732683\n",
      "  0.35415425286697927 -0.13960799967632723 -0.17634970032423464]\n",
      " [0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0\n",
      "  0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n",
      "  1.3823137309015021 0.969515976706386 0.33719795712834033\n",
      "  1.5662780418312758 0.9048711821226231 0.308275905898216\n",
      "  0.927181762237701 -1.4479623710112368 -1.2741212989964896\n",
      "  -1.72754945016184 0.42672373143441605 -1.179757470420891]]\n"
     ]
    }
   ],
   "source": [
    "# Print first 3 rows from the X test set\n",
    "print(X_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Print all rows from the Y test set\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the training set\n",
    "\n",
    "# Import the library for training the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance of the LogisticRegression class\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "\n",
    "# Train the Logistic Regression model on the training set\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import the library for encoding the categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define the data to be encoded\n",
    "cat_d = ['Poor','Self-Employed','None','Fair','Auto','Judgement','Debt Consolidation']\n",
    "\n",
    "# Create the one-hot encoder object\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder to the data\n",
    "encoder.fit([[x] for x in cat_d])\n",
    "\n",
    "# Transform the data into a one-hot encoded matrix\n",
    "cat_d_encoded = encoder.transform([[x] for x in cat_d]).toarray()\n",
    "\n",
    "# Print the resulting matrix\n",
    "print(cat_d_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 7 features, but ColumnTransformer is expecting 19 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m cat_d \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mPoor\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mSelf-Employed\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mFair\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mAuto\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mJudgement\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDebt Consolidation\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[39m# Transform the categorical data into numerical data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m cat_d \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(ct\u001b[39m.\u001b[39;49mtransform([cat_d]))\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(cat_d)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Encode the categorical data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# cat_d = np.array([[cat_d]])\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# # Combine the categorical and numerical data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# new_d = cat_d + num_d\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carlos Alves\\OneDrive - Goldsmiths College\\dev\\code_final_project\\project-env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Carlos Alves\\OneDrive - Goldsmiths College\\dev\\code_final_project\\project-env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:798\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcolumns are missing: \u001b[39m\u001b[39m{\u001b[39;00mdiff\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    795\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     \u001b[39m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m    797\u001b[0m     \u001b[39m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    800\u001b[0m Xs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_transform(\n\u001b[0;32m    801\u001b[0m     X,\n\u001b[0;32m    802\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    805\u001b[0m     column_as_strings\u001b[39m=\u001b[39mfit_dataframe_and_transform_dataframe,\n\u001b[0;32m    806\u001b[0m )\n\u001b[0;32m    807\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_output(Xs)\n",
      "File \u001b[1;32mc:\\Users\\Carlos Alves\\OneDrive - Goldsmiths College\\dev\\code_final_project\\project-env\\lib\\site-packages\\sklearn\\base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 370\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 7 features, but ColumnTransformer is expecting 19 features as input."
     ]
    }
   ],
   "source": [
    "# Input the new data to predict the result\n",
    "\n",
    "# Declare a new data point with categorical data\n",
    "cat_d = ['Poor','Self-Employed','None','Fair','Auto','Judgement','Debt Consolidation']\n",
    "\n",
    "# Transform the categorical data into numerical data\n",
    "cat_d = np.array(ct.transform([cat_d]))\n",
    "\n",
    "# Encode the categorical data\n",
    "cat_d = np.array([[cat_d]])\n",
    "\n",
    "# Declare a new data point with numerical data\n",
    "num_d = [30971,11969,78,21,0,5,0,135,18627,25,4648,4719]\n",
    "\n",
    "# Combine the categorical and numerical data\n",
    "new_d = cat_d + num_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Predict a new result\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m classifier\u001b[39m.\u001b[39;49mpredict([[new_d]])\n",
      "File \u001b[1;32mc:\\Users\\Carlos Alves\\OneDrive - Goldsmiths College\\dev\\code_final_project\\project-env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 419\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[0;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    421\u001b[0m     indices \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Carlos Alves\\OneDrive - Goldsmiths College\\dev\\code_final_project\\project-env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    397\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    398\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 400\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    401\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[0;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mreshape(scores, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Carlos Alves\\OneDrive - Goldsmiths College\\dev\\code_final_project\\project-env\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\Carlos Alves\\OneDrive - Goldsmiths College\\dev\\code_final_project\\project-env\\lib\\site-packages\\sklearn\\utils\\validation.py:910\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    907\u001b[0m         )\n\u001b[0;32m    909\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n\u001b[0;32m    914\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    915\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead."
     ]
    }
   ],
   "source": [
    "# Predict a new result\n",
    "\n",
    "classifier.predict([[new_d]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07399a9b4a62e4c2eb93a877b8941afe4222eac8f395849197d7e356d4348282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
